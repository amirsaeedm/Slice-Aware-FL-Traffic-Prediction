{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0301582",
   "metadata": {},
   "source": [
    "Process Extra Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bab13c",
   "metadata": {},
   "source": [
    "1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f94fc",
   "metadata": {},
   "source": [
    "2. Read the file names to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5728e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "INPUT_DIR  = '../dataset/extraData'\n",
    "OUTPUT_DIR = '../dataset/processedData'\n",
    "SCALER_TYPE = 'standard'     # 'standard' or 'minmax'\n",
    "FREQ = '2T'                  # 2-minute resampling\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'scalers'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe67947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rebek\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RIY0329 20250813 …\n",
      "  -> wrote raw: ../dataset/processedData\\RIY0329_Processed_20250813.csv\n",
      "  -> wrote scaled: ../dataset/processedData\\RIY0329_Processed_20250813_scaled_standard.csv\n",
      "  -> saved scaler: ../dataset/processedData\\scalers\\RIY0329_20250813_standard.joblib\n",
      "Processing RIY0443 20250813 …\n",
      "  -> wrote raw: ../dataset/processedData\\RIY0443_Processed_20250813.csv\n",
      "  -> wrote scaled: ../dataset/processedData\\RIY0443_Processed_20250813_scaled_standard.csv\n",
      "  -> saved scaler: ../dataset/processedData\\scalers\\RIY0443_20250813_standard.joblib\n",
      "Processing RIY2110 20250813 …\n",
      "  -> wrote raw: ../dataset/processedData\\RIY2110_Processed_20250813.csv\n",
      "  -> wrote scaled: ../dataset/processedData\\RIY2110_Processed_20250813_scaled_standard.csv\n",
      "  -> saved scaler: ../dataset/processedData\\scalers\\RIY2110_20250813_standard.joblib\n",
      "Processing RIY2227 20250813 …\n",
      "  -> wrote raw: ../dataset/processedData\\RIY2227_Processed_20250813.csv\n",
      "  -> wrote scaled: ../dataset/processedData\\RIY2227_Processed_20250813_scaled_standard.csv\n",
      "  -> saved scaler: ../dataset/processedData\\scalers\\RIY2227_20250813_standard.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def extract_area_date(fname):\n",
    "    \"\"\"Parse area code and yyyymmdd from names like:\n",
    "       RIY0329_DL_DataTransmission20250813171446.csv\"\"\"\n",
    "    base = os.path.basename(fname)\n",
    "    m = re.match(r'^(RIY\\d+)_.*?(\\d{8})\\d{6}\\.csv$', base)\n",
    "    if m:\n",
    "        return m.group(1), m.group(2)  # ('RIY0329', '20250813')\n",
    "    return None, None\n",
    "\n",
    "def normalize_date(yyyymmdd):\n",
    "    return f'{yyyymmdd[:4]}-{yyyymmdd[4:6]}-{yyyymmdd[6:]}'\n",
    "\n",
    "def load_and_timestamp(path, yyyymmdd):\n",
    "    \"\"\"Read CSV where first col is HH:MM:SS, build full datetime 'time'.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    # First column is time-of-day\n",
    "    first_col = df.columns[0]\n",
    "    df = df.rename(columns={first_col: 'HH:MM:SS'})\n",
    "    base_date = normalize_date(yyyymmdd)\n",
    "    df['time'] = pd.to_datetime(base_date + ' ' + df['HH:MM:SS'].astype(str), errors='coerce')\n",
    "    df = df.dropna(subset=['time'])\n",
    "    return df\n",
    "\n",
    "def aggregate_dl(dl):\n",
    "    # Column name options (vendors differ)\n",
    "    down_col   = next((c for c in dl.columns if 'DLMACTHP' in c), None)\n",
    "    mcs_col    = 'DLSchMcs'\n",
    "    rb_col     = next((c for c in dl.columns if c in ['DLRb_Used','DLRb_Use','DLAvailRb']), None)\n",
    "    users_col  = 'DLSchUserNum'\n",
    "\n",
    "    agg = dl.resample(FREQ, on='time').agg({\n",
    "        down_col: 'mean',\n",
    "        mcs_col: ['mean', 'var'],\n",
    "        rb_col:  ['mean', 'var'],\n",
    "        users_col: 'sum'\n",
    "    })\n",
    "    agg.columns = ['down', 'mcs_down', 'mcs_down_var', 'rb_down', 'rb_down_var', 'rnti_count_dl']\n",
    "    # Convert down from Mbps → bps\n",
    "    agg['down'] = agg['down'] * 1_000_000\n",
    "    return agg\n",
    "\n",
    "def aggregate_ul(ul):\n",
    "    up_col     = next((c for c in ul.columns if 'UplinkMACTHP' in c), None)\n",
    "    mcs_col    = 'ULSchMcs'\n",
    "    rb_col     = 'UplinkRb'\n",
    "    users_col  = 'ULSchUserNum'\n",
    "\n",
    "    agg = ul.resample(FREQ, on='time').agg({\n",
    "        up_col: 'mean',\n",
    "        mcs_col: ['mean', 'var'],\n",
    "        rb_col:  ['mean', 'var'],\n",
    "        users_col: 'sum'\n",
    "    })\n",
    "    agg.columns = ['up', 'mcs_up', 'mcs_up_var', 'rb_up', 'rb_up_var', 'rnti_count_ul']\n",
    "\n",
    "    # Convert down from Mbps → bps\n",
    "    agg['up'] = agg['up'] * 1_000_000\n",
    "    return agg\n",
    "\n",
    "def fix_zeros_and_nans(df, cols_to_clean):\n",
    "    \"\"\"Treat isolated zeros as missing for continuous metrics, then fill.\"\"\"\n",
    "    for c in cols_to_clean:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        # Mark zeros as NaN only if the column has nonzero values elsewhere\n",
    "        if (df[c] == 0).any() and (df[c] != 0).any():\n",
    "            df.loc[df[c] == 0, c] = np.nan\n",
    "        # Forward/backward fill then median as last resort\n",
    "        df[c] = df[c].ffill().bfill()\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].median())\n",
    "    return df\n",
    "\n",
    "def get_scaler(scaler_type='standard'):\n",
    "    return StandardScaler() if scaler_type == 'standard' else MinMaxScaler()\n",
    "\n",
    "def scale_safe(df, feature_cols, scaler_type, scaler_path):\n",
    "    \"\"\"Scale features, skipping constant-variance columns; save scaler.\"\"\"\n",
    "    # Skip columns with all equal values (std ~ 0)\n",
    "    non_constant = [c for c in feature_cols if df[c].std(skipna=True) > 1e-12]\n",
    "    constant     = [c for c in feature_cols if c not in non_constant]\n",
    "\n",
    "    scaler = get_scaler(scaler_type)\n",
    "    df_scaled = df.copy()\n",
    "    if non_constant:\n",
    "        vals = scaler.fit_transform(df[non_constant].values)\n",
    "        df_scaled[non_constant] = vals\n",
    "        joblib.dump({'scaler': scaler,\n",
    "                     'columns': non_constant,\n",
    "                     'constant_cols': constant,\n",
    "                     'type': scaler_type}, scaler_path)\n",
    "    else:\n",
    "        # Nothing to scale, but still save an identity config\n",
    "        joblib.dump({'scaler': None,\n",
    "                     'columns': [],\n",
    "                     'constant_cols': constant,\n",
    "                     'type': scaler_type}, scaler_path)\n",
    "\n",
    "    # Copy constant columns as-is\n",
    "    for c in constant:\n",
    "        df_scaled[c] = df[c]\n",
    "    return df_scaled\n",
    "\n",
    "# -----------------------------\n",
    "# Pair UL/DL files by area+date\n",
    "# -----------------------------\n",
    "paths = [os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) if f.startswith('RIY') and f.endswith('.csv')]\n",
    "pairs = {}  # {(area, date): {'DL': path, 'UL': path}}\n",
    "for p in paths:\n",
    "    area, date = extract_area_date(p)\n",
    "    if not area or not date:\n",
    "        continue\n",
    "    key = (area, date)\n",
    "    d = pairs.setdefault(key, {})\n",
    "    if '_DL_' in p:\n",
    "        d['DL'] = p\n",
    "    elif '_UL_' in p:\n",
    "        d['UL'] = p\n",
    "\n",
    "# -----------------------------\n",
    "# Process each (area, date)\n",
    "# -----------------------------\n",
    "for (area, date), files in sorted(pairs.items()):\n",
    "    if 'DL' not in files or 'UL' not in files:\n",
    "        print(f'Skipping {area} {date}: missing DL or UL file')\n",
    "        continue\n",
    "\n",
    "    print(f'Processing {area} {date} …')\n",
    "\n",
    "    dl = load_and_timestamp(files['DL'], date)\n",
    "    ul = load_and_timestamp(files['UL'], date)\n",
    "\n",
    "    dl_agg = aggregate_dl(dl)\n",
    "    ul_agg = aggregate_ul(ul)\n",
    "\n",
    "    combined = dl_agg.join(ul_agg, how='outer').reset_index()  # has 'time'\n",
    "    combined['rnti_count'] = combined['rnti_count_dl'].fillna(0) + combined['rnti_count_ul'].fillna(0)\n",
    "\n",
    "    # Keep exactly the forecasting schema\n",
    "    cols_final = [\n",
    "        'time', 'down', 'up', 'rnti_count',\n",
    "        'mcs_down', 'mcs_down_var',\n",
    "        'mcs_up', 'mcs_up_var',\n",
    "        'rb_down', 'rb_down_var',\n",
    "        'rb_up', 'rb_up_var'\n",
    "    ]\n",
    "    for c in cols_final:\n",
    "        if c not in combined.columns:\n",
    "            combined[c] = np.nan\n",
    "    combined = combined[cols_final].sort_values('time')\n",
    "\n",
    "    # Clean zeros / fill NaNs for continuous metrics\n",
    "    cont_cols = ['down','up','mcs_down','mcs_down_var','mcs_up','mcs_up_var',\n",
    "                 'rb_down','rb_down_var','rb_up','rb_up_var']\n",
    "    combined = fix_zeros_and_nans(combined, cont_cols)\n",
    "\n",
    "    # Save RAW (unscaled)\n",
    "    raw_out = os.path.join(OUTPUT_DIR, f'{area}_Processed_{date}.csv')\n",
    "    combined.to_csv(raw_out, index=False)\n",
    "    print(f'  -> wrote raw: {raw_out}')\n",
    "\n",
    "    # SCALE + NORMALIZE (save as separate file)\n",
    "    scaler_path = os.path.join(OUTPUT_DIR, 'scalers', f'{area}_{date}_{SCALER_TYPE}.joblib')\n",
    "    scaled = combined.copy()\n",
    "    scaled_features = cont_cols + ['rnti_count']  # include rnti_count too; drop if you prefer it unscaled\n",
    "    scaled = scale_safe(scaled, scaled_features, SCALER_TYPE, scaler_path)\n",
    "\n",
    "    norm_out = os.path.join(OUTPUT_DIR, f'{area}_Processed_{date}_scaled_{SCALER_TYPE}.csv')\n",
    "    scaled.to_csv(norm_out, index=False)\n",
    "    print(f'  -> wrote scaled: {norm_out}')\n",
    "    print(f'  -> saved scaler: {scaler_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
