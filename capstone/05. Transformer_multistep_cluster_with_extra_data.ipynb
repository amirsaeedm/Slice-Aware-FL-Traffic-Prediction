{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e057a4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rebek\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Add the parent directory of 'ml' to sys.path\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ml.models.transformer import TimeSeriesTransformer\n",
    "from ml.utils.data_utils import prepare_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689ec840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config (cluster dataset)\n",
    "# -----------------------\n",
    "args = Namespace(\n",
    "    data_path='../dataset/combined_with_cluster_feature_with_extraData.csv',  # <â€” clustered data\n",
    "    targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'],\n",
    "    num_lags=10,                  # encoder length (L)\n",
    "    forecast_steps=6,             # horizon (H)\n",
    "    test_size=0.2,                # 80/20 split\n",
    "    ignore_cols=None,\n",
    "    identifier='District',\n",
    "    nan_constant=0,\n",
    "    x_scaler='minmax',\n",
    "    y_scaler='minmax',\n",
    "    outlier_detection=True,\n",
    "    use_time_features=False,      # (kept off to mirror cluster runs)\n",
    "\n",
    "    # Transformer model hyperparams (match working eval config)\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1,\n",
    "\n",
    "    # Training\n",
    "    epochs=30,                    # you can bump this (e.g., 50) if needed\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.0,\n",
    "    grad_clip=1.0,\n",
    "    early_stopping_patience=8,    # stop if val loss doesn't improve\n",
    "\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_path='transformer_multistep_cluster_with_extra_data.pt',     # best checkpoint (weights)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf324db",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadaa3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (21708, 10, 7), y_train: (21708, 6, 5)\n",
      "X_test : (5427, 10, 7),  y_test : (5427, 6, 5)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "X_train, y_train, X_test, y_test, x_scaler, y_scaler, id_train, id_test = prepare_dataset(args)\n",
    "# Shapes\n",
    "# X_*: [N, L, D]\n",
    "# y_*: [N, H, T]\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test : {X_test.shape},  y_test : {y_test.shape}\")\n",
    "\n",
    "input_size  = X_train.shape[2]  # D\n",
    "output_size = y_train.shape[2]  # T\n",
    "L = X_train.shape[1]\n",
    "H = y_train.shape[1]\n",
    "assert L == args.num_lags and H == args.forecast_steps, \"Check num_lags/forecast_steps vs data.\"\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                  torch.tensor(y_train, dtype=torch.float32)),\n",
    "    batch_size=args.batch_size, shuffle=True, drop_last=False\n",
    ")\n",
    "val_loader = DataLoader(  # using test as validation for training loop; can split train if you prefer\n",
    "    TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                  torch.tensor(y_test, dtype=torch.float32)),\n",
    "    batch_size=args.batch_size, shuffle=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce12c5",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3a4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Build model\n",
    "# -----------------------\n",
    "model = TimeSeriesTransformer(\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    forecast_steps=args.forecast_steps,\n",
    "    d_model=args.d_model,\n",
    "    nhead=args.nhead,\n",
    "    num_encoder_layers=args.num_encoder_layers,\n",
    "    num_decoder_layers=args.num_decoder_layers,\n",
    "    dim_feedforward=args.dim_feedforward,\n",
    "    dropout=args.dropout,\n",
    ").to(args.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optional scheduler (cosine annealing works well for Transformers)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, args.epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68348829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/30] Train 0.081584 | Val 0.025754 | LR 9.97e-04\n",
      "[002/30] Train 0.016484 | Val 0.020275 | LR 9.89e-04\n",
      "[003/30] Train 0.013411 | Val 0.018425 | LR 9.76e-04\n",
      "[004/30] Train 0.012303 | Val 0.022164 | LR 9.57e-04\n",
      "[005/30] Train 0.011587 | Val 0.018709 | LR 9.33e-04\n",
      "[006/30] Train 0.011058 | Val 0.018258 | LR 9.05e-04\n",
      "[007/30] Train 0.010647 | Val 0.017922 | LR 8.72e-04\n",
      "[008/30] Train 0.010320 | Val 0.019124 | LR 8.35e-04\n",
      "[009/30] Train 0.010254 | Val 0.018054 | LR 7.94e-04\n",
      "[010/30] Train 0.010068 | Val 0.019695 | LR 7.50e-04\n",
      "[011/30] Train 0.010074 | Val 0.018987 | LR 7.03e-04\n",
      "[012/30] Train 0.009937 | Val 0.016832 | LR 6.55e-04\n",
      "[013/30] Train 0.009636 | Val 0.016893 | LR 6.04e-04\n",
      "[014/30] Train 0.009620 | Val 0.017520 | LR 5.52e-04\n",
      "[015/30] Train 0.009534 | Val 0.017957 | LR 5.00e-04\n",
      "[016/30] Train 0.009532 | Val 0.017898 | LR 4.48e-04\n",
      "[017/30] Train 0.009417 | Val 0.017607 | LR 3.96e-04\n",
      "[018/30] Train 0.009333 | Val 0.017102 | LR 3.45e-04\n",
      "[019/30] Train 0.009213 | Val 0.016982 | LR 2.97e-04\n",
      "[020/30] Train 0.009220 | Val 0.017097 | LR 2.50e-04\n",
      "Early stopping at epoch 20 (best=0.016832 @ 12)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Train / Val loops\n",
    "# -----------------------\n",
    "best_val = float('inf')\n",
    "best_epoch = -1\n",
    "no_improve = 0\n",
    "\n",
    "scaler_amp = torch.cuda.amp.GradScaler(enabled=(args.device == 'cuda'))\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(args.device), yb.to(args.device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(args.device == 'cuda')):\n",
    "            preds = model(xb)            # [B, H, T], trained on SCALED space\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "        scaler_amp.scale(loss).backward()\n",
    "        if args.grad_clip and args.grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "        scaler_amp.step(optimizer)\n",
    "        scaler_amp.update()\n",
    "\n",
    "        running += loss.item()\n",
    "\n",
    "    train_loss = running / max(1, len(train_loader))\n",
    "\n",
    "    # ---- Validate ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vloss = 0.0\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(args.device), yb.to(args.device)\n",
    "            preds = model(xb)\n",
    "            vloss += criterion(preds, yb).item()\n",
    "        val_loss = vloss / max(1, len(val_loader))\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"[{epoch:03d}/{args.epochs}] Train {train_loss:.6f} | Val {val_loss:.6f} | LR {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # ---- Early Stopping + Best Save ----\n",
    "    if val_loss < best_val - 1e-8:\n",
    "        best_val = val_loss\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), args.save_path)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= args.early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (best={best_val:.6f} @ {best_epoch})\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d121d7-5e0e-4173-8551-c80aaab67b27",
   "metadata": {},
   "source": [
    "## The Evaluation is carried out together with the other models in Notebook # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6beb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1a3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd8346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf2240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd383743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
