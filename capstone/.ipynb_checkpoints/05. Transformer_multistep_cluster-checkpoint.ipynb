{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Add the parent directory of 'ml' to sys.path\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ml.models.transformer import TimeSeriesTransformer\n",
    "from ml.utils.data_utils import prepare_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ec840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config (cluster dataset)\n",
    "# -----------------------\n",
    "args = Namespace(\n",
    "    data_path='../dataset/combined_with_cluster_feature.csv',  # <â€” clustered data\n",
    "    targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'],\n",
    "    num_lags=10,                  # encoder length (L)\n",
    "    forecast_steps=6,             # horizon (H)\n",
    "    test_size=0.2,                # 80/20 split\n",
    "    ignore_cols=None,\n",
    "    identifier='District',\n",
    "    nan_constant=0,\n",
    "    x_scaler='minmax',\n",
    "    y_scaler='minmax',\n",
    "    outlier_detection=True,\n",
    "    use_time_features=False,      # (kept off to mirror your cluster runs)\n",
    "\n",
    "    # Transformer model hyperparams (match your working eval config)\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1,\n",
    "\n",
    "    # Training\n",
    "    epochs=30,                    # you can bump this (e.g., 50) if needed\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.0,\n",
    "    grad_clip=1.0,\n",
    "    early_stopping_patience=8,    # stop if val loss doesn't improve\n",
    "\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_path='transformer_multistep_cluster.pt',     # best checkpoint (weights)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf324db",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadaa3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (21596, 10, 7), y_train: (21596, 6, 5)\n",
      "X_test : (5399, 10, 7),  y_test : (5399, 6, 5)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "X_train, y_train, X_test, y_test, x_scaler, y_scaler, id_train, id_test = prepare_dataset(args)\n",
    "# Shapes\n",
    "# X_*: [N, L, D]\n",
    "# y_*: [N, H, T]\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test : {X_test.shape},  y_test : {y_test.shape}\")\n",
    "\n",
    "input_size  = X_train.shape[2]  # D\n",
    "output_size = y_train.shape[2]  # T\n",
    "L = X_train.shape[1]\n",
    "H = y_train.shape[1]\n",
    "assert L == args.num_lags and H == args.forecast_steps, \"Check num_lags/forecast_steps vs data.\"\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                  torch.tensor(y_train, dtype=torch.float32)),\n",
    "    batch_size=args.batch_size, shuffle=True, drop_last=False\n",
    ")\n",
    "val_loader = DataLoader(  # using test as validation for training loop; can split train if you prefer\n",
    "    TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                  torch.tensor(y_test, dtype=torch.float32)),\n",
    "    batch_size=args.batch_size, shuffle=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce12c5",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3a4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Build model\n",
    "# -----------------------\n",
    "model = TimeSeriesTransformer(\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    forecast_steps=args.forecast_steps,\n",
    "    d_model=args.d_model,\n",
    "    nhead=args.nhead,\n",
    "    num_encoder_layers=args.num_encoder_layers,\n",
    "    num_decoder_layers=args.num_decoder_layers,\n",
    "    dim_feedforward=args.dim_feedforward,\n",
    "    dropout=args.dropout,\n",
    ").to(args.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optional scheduler (cosine annealing works well for Transformers)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, args.epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68348829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/30] Train 0.090166 | Val 0.016588 | LR 9.97e-04\n",
      "[002/30] Train 0.018644 | Val 0.014517 | LR 9.89e-04\n",
      "[003/30] Train 0.015601 | Val 0.013762 | LR 9.76e-04\n",
      "[004/30] Train 0.013994 | Val 0.013139 | LR 9.57e-04\n",
      "[005/30] Train 0.013238 | Val 0.012817 | LR 9.33e-04\n",
      "[006/30] Train 0.012681 | Val 0.013683 | LR 9.05e-04\n",
      "[007/30] Train 0.011880 | Val 0.013234 | LR 8.72e-04\n",
      "[008/30] Train 0.011510 | Val 0.012428 | LR 8.35e-04\n",
      "[009/30] Train 0.011380 | Val 0.012479 | LR 7.94e-04\n",
      "[010/30] Train 0.011299 | Val 0.012172 | LR 7.50e-04\n",
      "[011/30] Train 0.011180 | Val 0.011879 | LR 7.03e-04\n",
      "[012/30] Train 0.011079 | Val 0.012341 | LR 6.55e-04\n",
      "[013/30] Train 0.010695 | Val 0.011594 | LR 6.04e-04\n",
      "[014/30] Train 0.010544 | Val 0.013324 | LR 5.52e-04\n",
      "[015/30] Train 0.010559 | Val 0.012164 | LR 5.00e-04\n",
      "[016/30] Train 0.010446 | Val 0.011293 | LR 4.48e-04\n",
      "[017/30] Train 0.010391 | Val 0.012002 | LR 3.96e-04\n",
      "[018/30] Train 0.010334 | Val 0.011949 | LR 3.45e-04\n",
      "[019/30] Train 0.010139 | Val 0.012264 | LR 2.97e-04\n",
      "[020/30] Train 0.010114 | Val 0.012146 | LR 2.50e-04\n",
      "[021/30] Train 0.010124 | Val 0.011674 | LR 2.06e-04\n",
      "[022/30] Train 0.010047 | Val 0.011776 | LR 1.65e-04\n",
      "[023/30] Train 0.010011 | Val 0.012278 | LR 1.28e-04\n",
      "[024/30] Train 0.010012 | Val 0.011578 | LR 9.55e-05\n",
      "Early stopping at epoch 24 (best=0.011293 @ 16)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Train / Val loops\n",
    "# -----------------------\n",
    "best_val = float('inf')\n",
    "best_epoch = -1\n",
    "no_improve = 0\n",
    "\n",
    "scaler_amp = torch.cuda.amp.GradScaler(enabled=(args.device == 'cuda'))\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(args.device), yb.to(args.device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(args.device == 'cuda')):\n",
    "            preds = model(xb)            # [B, H, T], trained on SCALED space\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "        scaler_amp.scale(loss).backward()\n",
    "        if args.grad_clip and args.grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "        scaler_amp.step(optimizer)\n",
    "        scaler_amp.update()\n",
    "\n",
    "        running += loss.item()\n",
    "\n",
    "    train_loss = running / max(1, len(train_loader))\n",
    "\n",
    "    # ---- Validate ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vloss = 0.0\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(args.device), yb.to(args.device)\n",
    "            preds = model(xb)\n",
    "            vloss += criterion(preds, yb).item()\n",
    "        val_loss = vloss / max(1, len(val_loader))\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"[{epoch:03d}/{args.epochs}] Train {train_loss:.6f} | Val {val_loss:.6f} | LR {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # ---- Early Stopping + Best Save ----\n",
    "    if val_loss < best_val - 1e-8:\n",
    "        best_val = val_loss\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), args.save_path)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= args.early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (best={best_val:.6f} @ {best_epoch})\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d121d7-5e0e-4173-8551-c80aaab67b27",
   "metadata": {},
   "source": [
    "## The Evaluation is carried out together with the other models in Notebook # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6beb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1a3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd8346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf2240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd383743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
